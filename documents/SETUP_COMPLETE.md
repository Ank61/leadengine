# ğŸ‰ Scrape Job API - Setup Complete!

## âœ… What Was Created

### 1. **Database Schema** (`app/schemas/scrape_job.py`)
   - `ScrapeJobCreate` - Request schema for creating jobs
   - `ScrapeJobResponse` - Response schema with full job details
   - `ScrapeJobStatusResponse` - Response schema for status checks

### 2. **RabbitMQ Publisher** (`app/mq/publisher.py`)
   - `RabbitMQPublisher` class for managing connections
   - `publish_scrape_job()` function for sending jobs to queue
   - Automatic connection management and error handling

### 3. **API Endpoints** (`app/api/v1/endpoints/scrape_jobs.py`)
   - `POST /api/v1/jobs/scrape` - Create new scrape job
   - `GET /api/v1/jobs/scrape/{job_id}` - Get job status
   - `GET /api/v1/jobs/scrape/user/{user_id}` - List user's jobs

### 4. **Updated Worker** (`workers/scraper_worker.py`)
   - Fixed to use correct model names (`RawLead` instead of `RawScrapedData`)
   - Proper timestamp tracking (`started_at`, `completed_at`)
   - Better error handling and logging

### 5. **Documentation**
   - `API_SCRAPE_JOBS.md` - Complete API documentation

---

## ğŸš€ How to Use

### Step 1: Start the API Server
```bash
cd backend
.\venv\Scripts\python.exe -m uvicorn app.main:app --reload
```

**Server is now running at:** `http://localhost:8000`

### Step 2: Open Swagger UI
Navigate to: **http://localhost:8000/docs**

### Step 3: Create a Scrape Job

1. Find the `POST /api/v1/jobs/scrape` endpoint
2. Click "Try it out"
3. Use this payload:

```json
{
  "user_id": "123e4567-e89b-12d3-a456-426614174000",
  "source_type": "google",
  "search_query": "saas companies sydney",
  "filters": {}
}
```

4. Click "Execute"
5. You'll get a response with `job_id` and status `queued`

### Step 4: Start the Worker (Optional)

In a **new terminal**:
```bash
cd backend
.\venv\Scripts\python.exe workers/scraper_worker.py
```

The worker will:
- Connect to RabbitMQ
- Pick up the job from the queue
- Update status to `running`
- Process the scrape (currently returns dummy data)
- Save results to `raw_leads` table
- Update status to `completed`

### Step 5: Check Job Status

Use the `GET /api/v1/jobs/scrape/{job_id}` endpoint with the job_id from Step 3.

---

## ğŸ“Š API Flow

```
Frontend/Swagger
    â†“
POST /api/v1/jobs/scrape
    â†“
1. Create job in DB (status: queued)
    â†“
2. Send message to RabbitMQ
    â†“
Worker picks up message
    â†“
3. Update status to "running"
    â†“
4. Run scraper (fetch data)
    â†“
5. Save to raw_leads table
    â†“
6. Update status to "completed"
    â†“
GET /api/v1/jobs/scrape/{job_id}
    â†“
View results
```

---

## ğŸ”§ Current Status

âœ… **API Server:** Running on http://localhost:8000
âœ… **Swagger Docs:** http://localhost:8000/docs
âœ… **Database Tables:** Created
âœ… **Endpoints:** Ready to use
â¸ï¸ **Worker:** Not started (optional - start manually)
â¸ï¸ **RabbitMQ:** Needs to be running for worker

---

## ğŸ“ Test Without Worker

You can test the API **without starting the worker**:

1. Create a job â†’ Status will be `queued`
2. Job will stay in `queued` state
3. Message will be in RabbitMQ queue waiting for worker

When you start the worker later, it will process all queued jobs.

---

## ğŸ¯ Next Steps

1. **Test the API** in Swagger (http://localhost:8000/docs)
2. **Start RabbitMQ** if you want to test the full flow
3. **Start the worker** to process jobs
4. **Implement real scraper logic** in `workers/scraper_worker.py`

---

## ğŸ“š Available Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/jobs/scrape` | Create new scrape job |
| GET | `/api/v1/jobs/scrape/{job_id}` | Get job status |
| GET | `/api/v1/jobs/scrape/user/{user_id}` | List user's jobs |
| GET | `/db-check` | Check database connection |
| GET | `/api/v1/health` | Health check |

---

## ğŸ› Troubleshooting

**API won't start:**
- Check if port 8000 is already in use
- Verify virtual environment is activated
- Check database connection in `.env`

**Worker won't start:**
- Ensure RabbitMQ is running
- Check RabbitMQ credentials in `.env`
- Verify `aio-pika` is installed

**Job stays in "queued":**
- Worker is not running
- RabbitMQ is not running
- Check worker logs for errors

---

## ğŸŠ You're All Set!

Your scrape job API is ready to use! Test it in Swagger and let me know if you need any adjustments.
