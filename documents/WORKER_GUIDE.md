# RabbitMQ Consumer Worker - Point-to-Point Pattern

## ğŸ¯ How It Works

### **It's a LISTENER, NOT a Scheduler!**

The worker uses an **event-driven, point-to-point pattern**:

```
âœ… Event-Driven Listener (What we use)
   - Worker continuously listens to the queue
   - Processes messages IMMEDIATELY when they arrive
   - No polling, no cron jobs, no 10-minute intervals
   - Real-time processing

âŒ Job Scheduler (What we DON'T use)
   - Runs every X minutes
   - Checks for new jobs
   - Slow, inefficient
   - Not real-time
```

---

## ğŸ“Š Message Flow

```
1. API receives request
   â†“
2. API creates job in DB (status: queued)
   â†“
3. API sends message to RabbitMQ queue
   â†“
4. Worker IMMEDIATELY receives message (listening 24/7)
   â†“
5. Worker updates job status to "running"
   â†“
6. Worker processes the scrape
   â†“
7. Worker saves results to raw_leads table
   â†“
8. Worker updates job status to "completed"
   â†“
9. Worker continues listening for next message
```

---

## ğŸš€ Starting the Worker

### **Option 1: Run Directly**
```bash
cd backend
.\venv\Scripts\python.exe workers/scraper_worker.py
```

### **Option 2: Run in Background (Windows)**
```powershell
# Start in new window
Start-Process powershell -ArgumentList "cd backend; .\venv\Scripts\python.exe workers/scraper_worker.py"
```

### **Option 3: Run as Service (Production)**
Use a process manager like:
- **Windows:** NSSM (Non-Sucking Service Manager)
- **Linux:** systemd, supervisor
- **Docker:** Run as container

---

## ğŸ§ What You'll See When Worker Starts

```
============================================================
SCRAPE JOB WORKER
============================================================
Event-Driven RabbitMQ Consumer
Point-to-Point Pattern
============================================================

ğŸ”Œ Connecting to RabbitMQ...
   Host: localhost:5672
   VHost: /
âœ“ Connected to RabbitMQ successfully!

============================================================
ğŸ§ WORKER STARTED - LISTENING FOR MESSAGES
============================================================
Queue: scrape_jobs
Prefetch: 1 message(s) at a time
Pattern: Point-to-Point (Event-Driven Listener)

ğŸ’¡ This worker will process messages IMMEDIATELY as they arrive.
   No polling or cron jobs needed!

â³ Waiting for messages... (Press Ctrl+C to stop)
============================================================
```

---

## ğŸ“¨ When a Message Arrives

```
============================================================
ğŸ“¨ NEW MESSAGE RECEIVED
============================================================
Job ID: 987fcdeb-51a2-43d7-b123-456789abcdef
User ID: 123e4567-e89b-12d3-a456-426614174000
Industry: SaaS
Geography: Sydney, Australia
Keywords: ['B2B', 'enterprise', 'cloud']
Source Types: ['google', 'linkedin']
Search Query: saas companies sydney
============================================================

ğŸƒ Starting job processing...
âœ“ Job status updated to: RUNNING

ğŸ” Running scraper...
âœ“ Scraper completed. Found 3 results

ğŸ’¾ Saving raw leads to database...
âœ“ Saved 3 new leads

============================================================
âœ… JOB COMPLETED SUCCESSFULLY
============================================================
Job ID: 987fcdeb-51a2-43d7-b123-456789abcdef
Status: completed
Total Found: 3
Total Processed: 3
Duration: 2.15 seconds
============================================================
```

---

## ğŸ”„ Point-to-Point Pattern Explained

### **What is Point-to-Point?**

- **One Queue** â†’ `scrape_jobs`
- **Multiple Producers** â†’ API endpoints (can have many)
- **Multiple Consumers** â†’ Workers (can scale horizontally)
- **One Message â†’ One Consumer** â†’ Each message processed by exactly ONE worker

### **Benefits:**

1. **Load Balancing** - Multiple workers share the load
2. **Fault Tolerance** - If one worker dies, others continue
3. **Scalability** - Add more workers to process faster
4. **Guaranteed Processing** - Each message processed exactly once

### **Example with Multiple Workers:**

```
API â†’ [Message 1] â†’ Queue â†’ Worker 1 (processes)
API â†’ [Message 2] â†’ Queue â†’ Worker 2 (processes)
API â†’ [Message 3] â†’ Queue â†’ Worker 1 (processes)
API â†’ [Message 4] â†’ Queue â†’ Worker 3 (processes)
```

---

## ğŸ”§ Worker Features

### **1. Event-Driven Processing**
- Listens 24/7 to the queue
- Processes messages immediately
- No polling or delays

### **2. Deduplication**
- Checks for duplicate leads using hash
- Skips duplicates automatically
- Tracks duplicate count

### **3. Status Tracking**
- Updates job status in real-time
- Tracks: queued â†’ running â†’ completed/failed
- Records timestamps (started_at, completed_at)

### **4. Error Handling**
- Catches and logs errors
- Updates job status to "failed" on error
- Continues processing next messages

### **5. Graceful Shutdown**
- Press Ctrl+C to stop
- Closes RabbitMQ connection cleanly
- Finishes current message before stopping

### **6. Detailed Logging**
- Shows all job criteria
- Progress updates
- Success/failure summaries

---

## ğŸ¯ Scaling Workers

### **Run Multiple Workers:**

```bash
# Terminal 1
.\venv\Scripts\python.exe workers/scraper_worker.py

# Terminal 2
.\venv\Scripts\python.exe workers/scraper_worker.py

# Terminal 3
.\venv\Scripts\python.exe workers/scraper_worker.py
```

**Result:** Messages distributed across all 3 workers automatically!

---

## ğŸ› ï¸ Configuration

Edit these constants in `scraper_worker.py`:

```python
MAX_RETRIES = 3           # Max retry attempts
QUEUE_NAME = "scrape_jobs"  # Queue to listen to
PREFETCH_COUNT = 1        # Messages to process at once
```

**PREFETCH_COUNT:**
- `1` = Process one message at a time (safer)
- `5` = Process up to 5 messages concurrently (faster, more memory)

---

## ğŸ“ Implementing Real Scraper Logic

Replace the `run_scraper()` method with your actual scraping logic:

```python
async def run_scraper(self, payload: dict):
    """Run actual scraper logic"""
    
    # Extract criteria
    industry = payload.get("industry")
    geography = payload.get("geography")
    keywords = payload.get("keywords", [])
    source_types = payload.get("source_types", [])
    search_query = payload.get("search_query")
    
    results = []
    
    # Scrape from each source type
    for source in source_types:
        if source == "google":
            results.extend(await scrape_google(search_query, industry, geography))
        elif source == "linkedin":
            results.extend(await scrape_linkedin(search_query, industry, geography))
        elif source == "crunchbase":
            results.extend(await scrape_crunchbase(search_query, industry, geography))
    
    # Filter by keywords
    if keywords:
        results = filter_by_keywords(results, keywords)
    
    return results
```

---

## âœ… Testing the Worker

### **1. Start the Worker**
```bash
.\venv\Scripts\python.exe workers/scraper_worker.py
```

### **2. Create a Job via API**
Go to http://localhost:8000/docs and create a scrape job

### **3. Watch the Worker Console**
You'll see the message arrive and get processed in real-time!

### **4. Check the Database**
```sql
-- Check job status
SELECT id, status, total_found, total_processed, created_at, completed_at 
FROM scrape_jobs 
ORDER BY created_at DESC 
LIMIT 5;

-- Check raw leads
SELECT id, scrape_job_id, raw_payload 
FROM raw_leads 
ORDER BY scraped_at DESC 
LIMIT 5;
```

---

## ğŸŠ Summary

âœ… **Event-Driven** - Processes messages immediately
âœ… **Point-to-Point** - One message â†’ One worker
âœ… **Scalable** - Run multiple workers
âœ… **Real-Time** - No delays or polling
âœ… **Fault Tolerant** - Handles errors gracefully
âœ… **Production Ready** - Graceful shutdown, logging, deduplication

**No cron jobs or schedulers needed!** The worker listens 24/7 and processes messages as they arrive. ğŸš€
